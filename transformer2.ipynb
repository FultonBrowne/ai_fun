{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using example from:\n",
    "https://huggingface.co/microsoft/DialoGPT-medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelWithLMHead\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pull models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ai_fun/transformer2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m#model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m#tokenizer = AutoTokenizer.from_pretrained(\"r3dhummingbird/DialoGPT-medium-joshua\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m \u001b[39m#model = AutoModelWithLMHead.from_pretrained(\"r3dhummingbird/DialoGPT-medium-joshua\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbigscience/T0_3B\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f66756e/workspaces/ai_fun/transformer2.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbigscience/T0_3B\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:588\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=585'>586</a>\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=586'>587</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=587'>588</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=588'>589</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py?line=589'>590</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1783\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1779'>1780</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1780'>1781</a>\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1782'>1783</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1783'>1784</a>\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1784'>1785</a>\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1785'>1786</a>\u001b[0m     init_configuration,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1786'>1787</a>\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1787'>1788</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1788'>1789</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1789'>1790</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1790'>1791</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1928\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1925'>1926</a>\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1926'>1927</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1927'>1928</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1928'>1929</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1929'>1930</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1930'>1931</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1931'>1932</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=1932'>1933</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:134\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=127'>128</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=128'>129</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=129'>130</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=130'>131</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=131'>132</a>\u001b[0m         )\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=133'>134</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=134'>135</a>\u001b[0m     vocab_file,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=135'>136</a>\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=136'>137</a>\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=137'>138</a>\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=138'>139</a>\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=139'>140</a>\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=140'>141</a>\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=141'>142</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=142'>143</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=144'>145</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=145'>146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:118\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=115'>116</a>\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=116'>117</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=117'>118</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=118'>119</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=119'>120</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=120'>121</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=121'>122</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=122'>123</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=123'>124</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=125'>126</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py?line=127'>128</a>\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"r3dhummingbird/DialoGPT-medium-joshua\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"r3dhummingbird/DialoGPT-medium-joshua\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"r3dhummingbird/DialoGPT-medium-joshua\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"conversational\": {\n",
      "      \"max_length\": 1000\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hey there.\n",
      "DialoGPT: Pretty good. You?\n",
      "DialoGPT: Thanks. Now let's get to the bottom of this\n",
      "DialoGPT: How many questions do you have\n",
      "DialoGPT: !\n",
      "DialoGPT: To be with you\n",
      "DialoGPT: You're very sharp,.\n",
      "DialoGPT: ... Joshua.\n",
      "DialoGPT: Nice to meet ya, and yours too.\n",
      "DialoGPT: ...Hee hee. Why don't I return all the memories I've been holding on\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "while True:\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    convinput = input(\">> User:\")\n",
    "    if(convinput == \"stop\"):\n",
    "        break\n",
    "    new_user_input_ids = tokenizer.encode(convinput + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    # '\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "    ++step"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
