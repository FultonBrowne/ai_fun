{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday (prototype 2)\n",
    "A dynamic NN based assistant and framework\n",
    "\n",
    "rewritten with a Dynamic NN framework to make my life a little easier. If you would like to see a half built version in tensorflow checkout friday.ipynb\n",
    "\n",
    "NOTE: quite a bit of this notebook rips off https://github.com/ml-jku/hopfield-layers/blob/master/examples/bit_pattern/bit_pattern_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relies on the non pip package hopefield-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/ml-jku/hopfield-layers\n",
      "  Cloning https://github.com/ml-jku/hopfield-layers to /tmp/pip-req-build-xa5ilh5b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ml-jku/hopfield-layers /tmp/pip-req-build-xa5ilh5b\n",
      "  Resolved https://github.com/ml-jku/hopfield-layers to commit f56f929c95b77a070ae675ea4f56b6d54d36e730\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /home/fulton/.local/lib/python3.9/site-packages (from hopfield-layers==1.0.2) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/fulton/.local/lib/python3.9/site-packages (from hopfield-layers==1.0.2) (1.22.2)\n",
      "Requirement already satisfied: typing-extensions in /home/fulton/.local/lib/python3.9/site-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/ml-jku/hopfield-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules used e.g. for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importing Hopfield-specific modules.\n",
    "from hflayers import Hopfield\n",
    "\n",
    "# Import auxiliary modules.\n",
    "from distutils.version import LooseVersion\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Importing PyTorch specific modules.\n",
    "from torch import Tensor\n",
    "from torch.nn import Flatten, Linear, Module\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set()\n",
    "sns.set_theme(style=\"dark\") # prioritys lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Auxiliaries\n",
    "Before digging into Hopfield-based networks, a few auxiliary variables and functions need to be defined. This is nothing special with respect to Hopfield-based networks, but rather common preparation work of (almost) every machine learning setting (e.g. definition of a data loader as well as a training loop). We will see, that this comprises the most work of this whole demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [3, 3] # the input shape used in this notebook\n",
    "output_shape = [3, 3] # the output shape used in this notebook\n",
    "topic_shape = [3, 3]\n",
    "zeros = torch.zeros(input_shape)\n",
    "ones = torch.ones(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(r'cuda:0' if torch.cuda.is_available() else r'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bit_pattern_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/fulton/code/ai_fun/friday2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fulton/code/ai_fun/friday2.ipynb#ch0000015?line=8'>9</a>\u001b[0m \u001b[39m# Create data loader of training set.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fulton/code/ai_fun/friday2.ipynb#ch0000015?line=9'>10</a>\u001b[0m sampler_train \u001b[39m=\u001b[39m SubsetRandomSampler(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m2048\u001b[39m \u001b[39m-\u001b[39m \u001b[39m256\u001b[39m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fulton/code/ai_fun/friday2.ipynb#ch0000015?line=10'>11</a>\u001b[0m data_loader_train \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mbit_pattern_set, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, sampler\u001b[39m=\u001b[39msampler_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fulton/code/ai_fun/friday2.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39m# Create data loader of validation set.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fulton/code/ai_fun/friday2.ipynb#ch0000015?line=13'>14</a>\u001b[0m sampler_eval \u001b[39m=\u001b[39m SubsetRandomSampler(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m256\u001b[39m)) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m2048\u001b[39m \u001b[39m-\u001b[39m \u001b[39m256\u001b[39m, \u001b[39m2048\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bit_pattern_set' is not defined"
     ]
    }
   ],
   "source": [
    "#bit_pattern_set = BitPatternSet(\n",
    "#    num_bags=2048,\n",
    "#    num_instances=16,\n",
    "#    num_signals=8,\n",
    "#    num_signals_per_bag=1,\n",
    "#    num_bits=8)\n",
    "log_dir = f'resources/'\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network: Module,\n",
    "                optimiser: AdamW,\n",
    "                data_loader: DataLoader\n",
    "               ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Execute one training epoch.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader: data loader instance providing training data\n",
    "    :return: tuple comprising training loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    losses, accuracies = [], []\n",
    "    for sample_data in data_loader:\n",
    "        data, target = sample_data[r'data'], sample_data[r'target']\n",
    "        data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "        # Process data by Hopfield-based network.\n",
    "        model_output = network.forward(input=data)\n",
    "\n",
    "        # Update network parameters.\n",
    "        optimiser.zero_grad()\n",
    "        loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(parameters=network.parameters(), max_norm=1.0, norm_type=2)\n",
    "        optimiser.step()\n",
    "\n",
    "        # Compute performance measures of current model.\n",
    "        accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "        accuracies.append(accuracy.detach().item())\n",
    "        losses.append(loss.detach().item())\n",
    "    \n",
    "    # Report progress of training procedure.\n",
    "    return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def eval_iter(network: Module,\n",
    "              data_loader: DataLoader\n",
    "             ) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the current model.\n",
    "    \n",
    "    :param network: network instance to evaluate\n",
    "    :param data_loader: data loader instance providing validation data\n",
    "    :return: tuple comprising validation loss as well as accuracy\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, accuracies = [], []\n",
    "        for sample_data in data_loader:\n",
    "            data, target = sample_data[r'data'], sample_data[r'target']\n",
    "            data, target = data.to(device=device), target.to(device=device)\n",
    "\n",
    "            # Process data by Hopfield-based network.\n",
    "            model_output = network.forward(input=data)\n",
    "            loss = binary_cross_entropy_with_logits(input=model_output, target=target, reduction=r'mean')\n",
    "\n",
    "            # Compute performance measures of current model.\n",
    "            accuracy = (model_output.sigmoid().round() == target).to(dtype=torch.float32).mean()\n",
    "            accuracies.append(accuracy.detach().item())\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # Report progress of validation procedure.\n",
    "        return (sum(losses) / len(losses), sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n",
    "def operate(network: Module,\n",
    "            optimiser: AdamW,\n",
    "            data_loader_train: DataLoader,\n",
    "            data_loader_eval: DataLoader,\n",
    "            num_epochs: int = 1\n",
    "           ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Train the specified network by gradient descent using backpropagation.\n",
    "    \n",
    "    :param network: network instance to train\n",
    "    :param optimiser: optimiser instance responsible for updating network parameters\n",
    "    :param data_loader_train: data loader instance providing training data\n",
    "    :param data_loader_eval: data loader instance providing validation data\n",
    "    :param num_epochs: amount of epochs to train\n",
    "    :return: data frame comprising training as well as evaluation performance\n",
    "    \"\"\"\n",
    "    losses, accuracies = {r'train': [], r'eval': []}, {r'train': [], r'eval': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train network.\n",
    "        performance = train_epoch(network, optimiser, data_loader_train)\n",
    "        losses[r'train'].append(performance[0])\n",
    "        accuracies[r'train'].append(performance[1])\n",
    "        \n",
    "        # Evaluate current model.\n",
    "        performance = eval_iter(network, data_loader_eval)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"epoch:\", epoch, \"of\", num_epochs, \"\\naccuracy:\", performance[1], \"\\nloss:\", performance[0])\n",
    "        losses[r'eval'].append(performance[0])\n",
    "        accuracies[r'eval'].append(performance[1])\n",
    "    \n",
    "    # Report progress of training and validation procedures.\n",
    "    return pd.DataFrame(losses), pd.DataFrame(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(loss: pd.DataFrame,\n",
    "                     accuracy: pd.DataFrame,\n",
    "                     log_file: str\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot and save loss and accuracy.\n",
    "    \n",
    "    :param loss: loss to be plotted\n",
    "    :param accuracy: accuracy to be plotted\n",
    "    :param log_file: target file for storing the resulting plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    \n",
    "    loss_plot = sns.lineplot(data=loss, ax=ax[0])\n",
    "    loss_plot.set(xlabel=r'Epoch', ylabel=r'Cross-entropy Loss')\n",
    "    \n",
    "    accuracy_plot = sns.lineplot(data=accuracy, ax=ax[1])\n",
    "    accuracy_plot.set(xlabel=r'Epoch', ylabel=r'Accuracy')\n",
    "    \n",
    "    ax[1].yaxis.set_label_position(r'right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(log_file)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Friday DataFrame class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FridayDataset(Dataset):\n",
    "  def __init__(self, df, labels):\n",
    "    super().__init__()\n",
    "    self.df = df\n",
    "    self.topic = labels[0]\n",
    "    self.fact = labels[1]\n",
    "  def __len__(self):\n",
    "\t\t#print len(self.landmarks_frame)\n",
    "\t\t#return len(self.landmarks_frame)\n",
    "\t  return len(df.index)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.at[idx, self.topic], self.at[idx, self.fact]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Friday model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Friday(Module):\n",
    "    def __init__(self, input_size, num_instances):\n",
    "        super().__init__()\n",
    "        self.hopfield = Hopfield(\n",
    "            input_size=input_size,\n",
    "            hidden_size=8,\n",
    "            num_heads=8,\n",
    "            update_steps_max=3,\n",
    "            scaling=0.25)\n",
    "        self.flatten = Flatten()\n",
    "        self.output_projection = Linear(in_features=self.hopfield.output_size * num_instances, out_features=1)\n",
    "        self.flatten2 = Flatten(start_dim=0)\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.hopfield(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_projection(x)\n",
    "        x = self.flatten2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/beavers.csv\") #TODO add a 80/20 datset splitter \n",
    "labels = [\"animals\", \"fact\"]\n",
    "dataset = FridayDataset(df = df, labels = labels)\n",
    "data_loader = DataLoader(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader of training set.\n",
    "data_loader_train = DataLoader(dataset=train_dataset, batch_size=32)\n",
    "\n",
    "# Create data loader of validation set.\n",
    "data_loader_eval = DataLoader(dataset=test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Friday(input_size = bit_pattern_set.num_bits, num_instances = bit_pattern_set.num_instances).to(device=device)\n",
    "optimiser = AdamW(params=network.parameters(), lr=1e-3)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = operate(\n",
    "    network=network,\n",
    "    optimiser=optimiser,\n",
    "    data_loader_train=data_loader_train,\n",
    "    data_loader_eval=data_loader_eval,\n",
    "    num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(loss=losses, accuracy=accuracies, log_file=f'{log_dir}/hopfield_base.pdf')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
